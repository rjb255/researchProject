%!TEX root = ../thesis.tex
%*******************************************************************************
%****************************** Second Chapter *********************************
%*******************************************************************************

\chapter{Previous Work}


\graphicspath{{Chapter2/Figs/Vector/}{Chapter2/Figs/}}


Scores displayed in examples have been based on the entire data set. Although this usually leads to data leakage within machine learning, this is not a concern here as the true comparison comes from testing  \textit{intelligent} vs \textit{dumb} learning methods. In both of these cases, the model is kept identical, but the selection process in not. The baseline simply takes the first $n$ entries from the data set, with the \textit{intelligent} method described where required. A simple function has been used to present data as a means of demonstrating these models, with $x$ having two dimensions. The function for $y$ is shown in \ref{eq:phantom} and displayed graphically in Figure \ref{fig:phantom}.


\begin{figure}[H]
  \begin{center}
    \includegraphics{phantomOverview.pdf}
    \caption{Contour plot of the function used to demonstrate the algorithms presented in previous work. The crosses have been used to show the location of the 200 test data points used within this example.}
    \label{fig:phantom}
  \end{center}
\end{figure}

\begin{equation}
  y = \sin{(x_1)}^{10} + \cos{(10 + x_1 x_2)}\cos{(x_1)}
  \label{eq:phantom}
\end{equation}

\section{Active Learning}
\label{ch:Active Learning}

There are several schools of thought regarding active learning. These can be separated into two distinct categories: current data and future predictions. The former of these is computationally cheaper, as will be apparent on description.

\subsection{Current Data}

\subsubsection{Uncertainty Sampling and Regions of Disagreements}
\label{sec:Uncertainty Sampling}

The simplest is applicable to cases in which a certainty is provided with each prediction. \textcite{Set09} suggests selecting the data point with the largest uncertainty according to the current model. Using the dataset '', this is demonstrated in Figure~\ref{fig:rodPhantom} with the algorithm for deciding the next sample point given in Algorithm~\ref{alg:US}.

\begin{algorithm}[H]
  \KwData{$X_\mathrm{known}$, $Y_\mathrm{known}$, $X_\mathrm{unknown}$}
  \KwResult{Next $X$ to label}
  model = BayesianRidge()\;
  model.fit($X_\mathrm{known}$, $Y_\mathrm{known}$)\;
  standard\_deviation = model.standard\_deviation($X_\mathrm{unknown}$)\;
  \Return{max(standard\_deviation)}
  \caption{Uncertainty Sampling Selection}
  \label{alg:US}\SetAlgoLined
\end{algorithm}


\begin{figure}[H]
  \begin{center}
    \includegraphics[]{rodPhantom.pdf}
    \caption[]{The outcome of the investigating the areas of the highest uncertainty. A) Demonstrates the final set of points tested by the algorithm and B) shows the change in the mean squared error for the algorithm after each iteration.}
    \label{fig:rodPhantom}
  \end{center}
\end{figure}
Interestingly, Figure \ref{fig:rodPhantom}B shows how the mean squared error for the random sampling method performed to worse within the iterations tested. This is likely due to a bias in the use of linear models in fitting leading to large uncertainties surrounding areas with high curvature. Evidence to this is provided in \ref{fig:rodPhantom}A with a large proportion of the sampled points at areas of high curvature.

As addressed by \textcite{Set09}, this can be extended to any probabilistic model through \ref{eq:x_next1}. \textcite{Set09} also notes the use of information theory for probabilistic models(\ref{eq:info_uncertainty}), where $y_i$ refers to all possible categorisations for $x$. This derives from the principle that the greatest entropy requires the most information to encode, and thus the least certain. However, \textcite{Set09} fails to address non-probabilistic models in this instance, instead converting such models into probabilistic ones.

\begin{equation}
  \label{eq:x_next1}
  x_\mathrm{next}=\argmax_X{\left[s_{g(X)}\right]}
\end{equation}

In order to adapt non-probabilistic models into probabilistic ones, composite models may be used. These are an amalgamation of other models where the standard deviation of the individual models can be taken as the degree of certainty for a given point. Many authors have called this as minimising the region of disagreement as it attempts to produce a coherent hypothesis space. By minimising the region of disagreement between various models, a finer fit may be achieved. Indeed, this was the method used in Figure \ref{fig:rodPhantom}.

One way of achieving this, especially in a regression model where boundaries are not quite so distinct, is to declare $n$ models ${M = \{m_1,\ldots{}, m_n\}}$. Combining these allow for a model $\hat{m}$ to be defined with prediction $\hat{y}$, being the mean prediction of $M$, ${\frac{1}{n}\sum{y_i}}$ and a sample standard deviation $\hat{s}$ defined as the sample standard deviation of $y_i$. This standard deviation can be used as a measure of the disagreement between the models. Thus, using a method as in Section~\ref{sec:Uncertainty Sampling}.

\begin{equation}
  \label{eq:info_uncertainty}
  x_\mathrm{next}=\argmax_x{\left[-\sum_i{P(y_i|x)\ln{P(y_i|x)}}\right]}
\end{equation}

\subsubsection{Broad Knowledge Base}
A second form stems from information theory. Here, the aim is to produce an evenly dispersed $x$ allowing a well-informed knowledge base. This prevents poor model choice from influencing the algorithm as was seen in \ref{fig:rodPhantom}. There are two paths to proceed: density and nearest neighbours.

The former of these requires a definition of density in a sparsely populated space. As an analogy, although the density of a gas appears well-defined, it becomes non-smooth once the volume defined over is comparable to the distance between particles. Thus, a new definition is required.

Alternatively, nearest neighbour requires little explanation. $x_\mathrm{next}$ is the unlabelled data point furthest from any labelled data point.

\begin{equation}
  \label{eq:inverseSim}
  x_\mathrm{next}=\argmax_x{\left(\sum{\frac{1}{\simm{(x, x_i)}}}\right)}
\end{equation}

\begin{center}
  \includegraphics[width=100mm]{BKB_300_all3.pdf}
\end{center}

\subsubsection{Density Hotspots}
Conversely, a density weighted model has been suggested, as it escapes the introduction of error from outlier (i.e.\ data points far away from alternative data points). \textcite{Set08} suggest (\ref{eq:Settles_density}) which can be broken down into two parts: a function for selection, $\phi_A$, and a function for similarity, $\simm$. The former arises from  another method described in this section. The latter requires a function to describe the similarity between data points.

\begin{equation}
  \label{eq:Settles_density}
  x_\mathrm{next}=\argmax_x{\left[\phi_A(x)\times{\left(\frac{1}{U}\sum{\simm{(x, x_i)}}\right)}^\beta\right]}
\end{equation}

\textcite{Set08} admits that $\simm$ is open for interpretation. For simplicity, the average distance can be taken as ...


\subsection{Estimated Future}
These methods attempt to minimise a future attribute of the model. This works by predicting changes given with the inclusion of more data.

\subsubsection{Expected Model Change}

\section{Batch Active Learning}
Several naive methods are available here. Firstly, getting the top $N$ data points from a model described in Section. However, this method does not take into account the equivalence of the data points. This is extremely clear using the highest uncertainty method. Each method in Section[] has been modified to demonstrate this weakness.

\begin{center}
  \includegraphics[width=150mm]{BKB_10_120_all3.pdf}
\end{center}
\begin{center}
  \includegraphics[width=150mm]{US_10_120_all3.pdf}
\end{center}

It stands to reason that the area which has the highest uncertainty will see this for the data points nearest neighbours. Thus, this singular data point suffers the potential of being surrounded by $N-1$ other data points. The benefit this provides in fitting the model is thus extremely limited, and only slightly greater than if one data point had been chosen. A simple fix would be to simulate the model after 1 iteration, and select the next point from here. By doing this $N-1$ times, a better solution may be found, although this may prove to be computationally very expensive.

\section{Drug Data}
There are numerous data categories that can be used to represent a chemical in a suitable form for machine learning. Each of these methods have various strengths and weaknesses. Some are directly based upon the chemical structure.
\section{Physical Properties}
A selection of physical properties from chemicals are known, from melting points to solubility. Many of these provide important aspects for consideration and allow human scientists to predict interactions, especially when determining new drugs. These data are often reported in tables within textbooks such as Perry's [] or provided through software [chembl ...].
\\
Several of these data can be predicted through theoretical models, although the difficulty increases for larger molecules. For example,
\blindtext[1]
\section{Fingerprints}
Another methodology is to develop a fingerprint: a unique code based on the chemical structure, either of the atomic arrangement, or by the electron cloud distribution. The latter of these is more fundamental to the activity of molecules but far harder to calculate. Indeed,
...Morgan Fingerprints
\blindtext[1]
\section{Combining Drug Data with Active Learning}
\blindtext[1]